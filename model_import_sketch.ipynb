{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xloem/techsketball/blob/wip/model_import_sketch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH1Ld_vd9wyx",
        "outputId": "c40a7044-01cc-4cc2-935f-8fdfc5793548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.7/dist-packages (0.5.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.10.0+cu111)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.7/dist-packages (from deepspeed) (8.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed) (21.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.10.2.3)\n",
            "Requirement already satisfied: triton==1.0.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.62.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed) (5.4.8)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.7/dist-packages (from deepspeed) (3.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deepspeed) (3.10.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.1.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: jax>=0.2.21 in /usr/local/lib/python3.7/dist-packages (from flax) (0.2.25)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (0.12.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.21->flax) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.71+cuda111)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax->flax) (2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "fatal: destination path 'techsketball' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "import jax\n",
        "\n",
        "starting_model_path = 't5-base'#'t5-small'#'bigscience/T0pp'\n",
        "input_width = 512\n",
        "# these are not t5 parameters?\n",
        "train_batch_size = 20 # small for notebook\n",
        "per_device_batch_size = train_batch_size // jax.device_count()\n",
        "num_epochs = 10\n",
        "training_seed = 0\n",
        "learning_rate = 3e-4\n",
        "\n",
        "\n",
        "#!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip3 install deepspeed\n",
        "!pip3 install transformers\n",
        "!pip3 install flax\n",
        "!pip3 install sentencepiece\n",
        "!git clone https://github.com/xloem/techsketball && ln -s techsketball/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTGBjrXoX2eS",
        "outputId": "4a7b7590-2846-4ac4-822d-a9a540cbafa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 16 19:28:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    32W / 250W |    257MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GpuDevice(id=0, process_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import jax.tools.colab_tpu\n",
        "import jaxlib\n",
        "import os\n",
        "try:\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "  jaxlib.xla_extension.tpu_client()\n",
        "  backend = 'tpu'\n",
        "except:\n",
        "  try:\n",
        "    jaxlib.xla_extension.gpu_client()\n",
        "    backend = 'gpu'\n",
        "  except:\n",
        "    backend = 'cpu'\n",
        "import tensorflow as tf\n",
        "!nvidia-smi\n",
        "jax.local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qhJhFJfQAOXG"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, FlaxT5ForConditionalGeneration \n",
        "tokenizer = T5Tokenizer.from_pretrained(starting_model_path) # only for source, not for binary\n",
        "model = FlaxT5ForConditionalGeneration.from_pretrained(starting_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mZqc_JNhixjS"
      },
      "outputs": [],
      "source": [
        "# before data is generated we can import libraries to generate it from\n",
        "import jax, jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import flax\n",
        "import flax.training.common_utils\n",
        "import flax.training.train_state\n",
        "import tqdm\n",
        "import time\n",
        "import os\n",
        "# ...\n",
        "import transformers\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QnyDTDt_f1fE",
        "outputId": "6252ccb7-ed2a-4db8-edcb-0ac73bb3dfad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting training data ...\n"
          ]
        }
      ],
      "source": [
        "import find_pycode\n",
        "print('getting training data ...')\n",
        "tokenizerpfx = starting_model_path.replace('/','_') + '.'\n",
        "find_pycode.write_files('example.', tokenizerpfx, 512, tokenizer, 512, globals(), skip_if_exists = True)\n",
        "train_data = find_pycode.read_files('example.', tokenizerpfx, 512, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6qTNv8oZwbGS"
      },
      "outputs": [],
      "source": [
        "#from tokenizers import ByteLevelBPETokenizer\n",
        "#tokenizer = ByteLevelBPETokenizer()\n",
        "#tokenizer.train_from_iterator((str for bytes, str in data_tuples), vocab_size=model.config.vocab_size, min_frequency=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CxCgcJ0dzQY_",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sdJ1Ek3-_j37"
      },
      "outputs": [],
      "source": [
        "#import deepspeed\n",
        "#cmd_args = None\n",
        "#model_engine, optimizer, _, _ = deepspeed.initialize(args=cmd_args,\n",
        "#                                                     model=model,\n",
        "#                                                     model_parameters=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nTQyR1KO4Sbv"
      },
      "outputs": [],
      "source": [
        "num_train_steps = len(train_data['input_ids']) // train_batch_size * num_epochs\n",
        "\n",
        "rng = jax.random.PRNGKey(training_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gD_cvz5MebpC"
      },
      "outputs": [],
      "source": [
        "def batch_from_indices(dataset : dict, indices):\n",
        "  #print(dataset['input_ids'].shape, indices.shape)\n",
        "  result = {\n",
        "      key : jnp.stack(data[indices,:])\n",
        "      for key, data in dataset.items()\n",
        "  }\n",
        "  # this change could be already put in the dataset passed by the function that produces it\n",
        "  result['labels'] = result['decoder_input_ids']\n",
        "  result['decoder_input_ids'] = np.asarray(transformers.models.t5.modeling_flax_t5.shift_tokens_right(result['labels'], tokenizer.pad_token_id, model.config.decoder_start_token_id))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "m58ESSevKp6P",
        "lines_to_next_cell": 2,
        "outputId": "19801cbf-211d-43f5-e143-3b1ad233e1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiling _threefry_split (139683547432432) for args (ShapedArray(uint32[2]),).\n",
            "WARNING:absl:Compiling prim_fun (139683547921856) for args ().\n",
            "WARNING:absl:Compiling _shuffle (139683547227152) for args (ShapedArray(uint32[2]), ShapedArray(int32[41521])).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing initial batch to compile train step ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiling prim_fun (139683547659552) for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Compiling prim_fun (139683545817888) for args (ShapedArray(int32[41521]), ShapedArray(int32[1])).\n",
            "WARNING:absl:Compiling prim_fun (139683545817888) for args (ShapedArray(int32[20]),).\n",
            "WARNING:absl:Compiling prim_fun (139683545817888) for args (ShapedArray(uint8[20,512]),).\n",
            "WARNING:absl:Compiling prim_fun (139683547832880) for args (ShapedArray(uint8[20,1,512]),).\n",
            "WARNING:absl:Compiling prim_fun (139683545817888) for args (ShapedArray(uint16[20,512]),).\n",
            "WARNING:absl:Compiling prim_fun (139683547733920) for args (ShapedArray(uint16[20,1,512]),).\n",
            "WARNING:absl:Compiling prim_fun (139683547657232) for args (ShapedArray(uint16[20,512]), ShapedArray(int32[1])).\n",
            "WARNING:absl:Compiling prim_fun (139683548550720) for args (ShapedArray(uint16[20,511]),).\n",
            "WARNING:absl:Compiling prim_fun (139683547657232) for args (ShapedArray(uint8[20,512]),).\n",
            "WARNING:absl:Compiling prim_fun (139683547730320) for args (ShapedArray(uint16[20,512]),).\n",
            "WARNING:absl:Compiling train_step (139683547381104) for 1 devices with args (ShapedArray(int32[1]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,32,12]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,32,12]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,32128,768]), ShapedArray(int32[1]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,32,12]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,32,12]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,32128,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,32,12]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,32,12]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768,3072]), ShapedArray(float32[1,3072,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,768]), ShapedArray(float32[1,32128,768]), ShapedArray(int32[1]), ShapedArray(uint8[1,20,512]), ShapedArray(uint8[1,20,512]), ShapedArray(uint16[1,20,512]), ShapedArray(uint8[1,20,512]), ShapedArray(uint16[1,20,512]), ShapedArray(uint32[1,2])). (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Compiling _multi_slice (139683434473104) for args (ShapedArray(uint8[1,20,512]),).\n",
            "WARNING:absl:Compiling _multi_slice (139683433148576) for args (ShapedArray(uint16[1,20,512]),).\n",
            "WARNING:absl:Compiling _multi_slice (139683432872848) for args (ShapedArray(uint32[1,2]),).\n",
            "WARNING:absl:Compiling _mean (139676181242256) for args (ShapedArray(float32[]),).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.  First loss was 9.468738\n"
          ]
        }
      ],
      "source": [
        "# these are not t5 parameters?\n",
        "linear_decay_lr_schedule_fn = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)\n",
        "adamw = optax.adamw(learning_rate=linear_decay_lr_schedule_fn, b1=0.9, b2=0.98, eps=1e-8, weight_decay=0.01)\n",
        "state = flax.training.train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw)\n",
        "\n",
        "jax.config.update('jax_log_compiles', True)\n",
        "\n",
        "# from run_t5_mlm_flax.py\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "# Define gradient update step fn\n",
        "def train_step(state, batch, dropout_rng):#input_ids, attention_mask, labels, decoder_input_ids, decoder_attention_mask, dropout_rng):\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "    def loss_fn(params):\n",
        "        labels = batch.pop('labels')\n",
        "\n",
        "        #logits = state.apply_fn(\n",
        "        #    input_ids = input_ids,\n",
        "        #    attention_mask = attention_mask,\n",
        "        #    decoder_input_ids = decoder_input_ids,\n",
        "        #    decoder_attention_mask = decoder_attention_mask,\n",
        "        #    params = params,\n",
        "        #    dropout_rng = dropout_rng,\n",
        "        #    train = True\n",
        "        #).logits\n",
        "        logits = state.apply_fn(**batch, params = params, dropout_rng = dropout_rng, train = True).logits\n",
        "        #print(logits.shape)\n",
        "        #assert len(logits[-1]) == tokenizer.vocab_size\n",
        "        #logits = logits[0]\n",
        "\n",
        "        # logits, labels, padding_mask=batch['decoder_attention_mask', label_smoothing_factor=0]\n",
        "        # compute loss\n",
        "        loss = optax.softmax_cross_entropy(logits, flax.training.common_utils.onehot(labels, logits.shape[-1]))\n",
        "        padding_mask = batch['decoder_attention_mask']\n",
        "        loss = (loss * padding_mask).sum() / padding_mask.sum()\n",
        "\n",
        "        #loss = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True).loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    metrics = jax.lax.pmean(\n",
        "        {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n",
        "    )\n",
        "\n",
        "    return new_state, metrics, new_dropout_rng\n",
        "\n",
        "# Create parallel version of the train step\n",
        "p_train_step = jax.pmap(train_step, 'batch', donate_argnums=(0,), backend=backend)\n",
        "\n",
        "# Replicate the train state on each device\n",
        "state = flax.jax_utils.replicate(state)\n",
        "\n",
        "print('Performing initial batch to compile train step ...')\n",
        "rng, input_rng = jax.random.split(rng)\n",
        "num_train_samples = len(train_data['input_ids'])\n",
        "train_samples_idx = jax.random.permutation(input_rng, jnp.arange(num_train_samples))\n",
        "model_inputs = batch_from_indices(train_data, train_samples_idx[:train_batch_size])\n",
        "model_inputs = flax.training.common_utils.shard(model_inputs)\n",
        "state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rng=dropout_rngs)\n",
        "train_metric = flax.jax_utils.unreplicate(train_metric)\n",
        "print('Done.  First loss was', train_metric['loss'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doKKw-W345Zn",
        "outputId": "dd3510d2-b9b7-485a-8ef9-6ccad799bed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch ... :   0%|          | 0/10 [00:00<?, ?it/s]\n",
            "Training...:   0%|          | 0/2076 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [01:21<?, ?it/s]\n",
            "Training...:   0%|          | 0/2076 [01:21<?, ?it/s]\u001b[A\n",
            "Training...:   0%|          | 1/2076 [01:21<47:09:31, 81.82s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (0 | Loss: 5.585570335388184, Learning Rate: 0.0002999855496454984)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [03:08<?, ?it/s]\n",
            "Training...:   0%|          | 1/2076 [03:08<47:09:31, 81.82s/it]\u001b[A\n",
            "Training...:   0%|          | 2/2076 [03:08<55:32:38, 96.41s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (1 | Loss: 4.787548542022705, Learning Rate: 0.00029997111414559186)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [04:31<?, ?it/s]\n",
            "Training...:   0%|          | 2/2076 [04:31<55:32:38, 96.41s/it]\u001b[A\n",
            "Training...:   0%|          | 3/2076 [04:31<51:58:23, 90.26s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (2 | Loss: 4.411281585693359, Learning Rate: 0.00029995664954185486)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [06:02<?, ?it/s]\n",
            "Training...:   0%|          | 3/2076 [06:02<51:58:23, 90.26s/it]\u001b[A\n",
            "Training...:   0%|          | 4/2076 [06:02<52:04:04, 90.47s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (3 | Loss: 4.535261631011963, Learning Rate: 0.0002999422140419483)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [07:31<?, ?it/s]\n",
            "Training...:   0%|          | 4/2076 [07:30<52:04:04, 90.47s/it]\u001b[A\n",
            "Training...:   0%|          | 5/2076 [07:30<51:42:13, 89.88s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (4 | Loss: 4.089804172515869, Learning Rate: 0.0002999277494382113)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [09:12<?, ?it/s]\n",
            "Training...:   0%|          | 5/2076 [09:12<51:42:13, 89.88s/it]\u001b[A\n",
            "Training...:   0%|          | 6/2076 [09:12<53:51:23, 93.66s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (5 | Loss: 4.190586090087891, Learning Rate: 0.0002999133139383048)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [10:52<?, ?it/s]\n",
            "Training...:   0%|          | 6/2076 [10:52<53:51:23, 93.66s/it]\u001b[A\n",
            "Training...:   0%|          | 7/2076 [10:52<55:02:43, 95.78s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (6 | Loss: 4.132073879241943, Learning Rate: 0.0002998988493345678)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [12:19<?, ?it/s]\n",
            "Training...:   0%|          | 7/2076 [12:19<55:02:43, 95.78s/it]\u001b[A\n",
            "Training...:   0%|          | 8/2076 [12:19<53:24:15, 92.97s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (7 | Loss: 4.202601909637451, Learning Rate: 0.00029988441383466125)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [13:49<?, ?it/s]\n",
            "Training...:   0%|          | 8/2076 [13:49<53:24:15, 92.97s/it]\u001b[A\n",
            "Training...:   0%|          | 9/2076 [13:49<52:50:06, 92.02s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (8 | Loss: 3.6492910385131836, Learning Rate: 0.00029986994923092425)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [15:24<?, ?it/s]\n",
            "Training...:   0%|          | 9/2076 [15:24<52:50:06, 92.02s/it]\u001b[A\n",
            "Training...:   0%|          | 10/2076 [15:24<53:26:48, 93.13s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (9 | Loss: 3.7850053310394287, Learning Rate: 0.0002998555137310177)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [16:50<?, ?it/s]\n",
            "Training...:   0%|          | 10/2076 [16:50<53:26:48, 93.13s/it]\u001b[A\n",
            "Training...:   1%|          | 11/2076 [16:50<52:12:30, 91.02s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (10 | Loss: 3.92744779586792, Learning Rate: 0.0002998410491272807)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [18:15<?, ?it/s]\n",
            "Training...:   1%|          | 11/2076 [18:15<52:12:30, 91.02s/it]\u001b[A\n",
            "Training...:   1%|          | 12/2076 [18:15<51:07:53, 89.18s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (11 | Loss: 3.56469464302063, Learning Rate: 0.00029982661362737417)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [19:57<?, ?it/s]\n",
            "Training...:   1%|          | 12/2076 [19:57<51:07:53, 89.18s/it]\u001b[A\n",
            "Training...:   1%|          | 13/2076 [19:57<53:12:36, 92.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (12 | Loss: 3.942274808883667, Learning Rate: 0.0002998121490236372)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [21:23<?, ?it/s]\n",
            "Training...:   1%|          | 13/2076 [21:22<53:12:36, 92.85s/it]\u001b[A\n",
            "Training...:   1%|          | 14/2076 [21:22<51:58:14, 90.73s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (13 | Loss: 3.501882791519165, Learning Rate: 0.00029979771352373064)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [22:47<?, ?it/s]\n",
            "Training...:   1%|          | 14/2076 [22:47<51:58:14, 90.73s/it]\u001b[A\n",
            "Training...:   1%|          | 15/2076 [22:47<50:47:09, 88.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (14 | Loss: 3.569455623626709, Learning Rate: 0.00029978324891999364)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [24:20<?, ?it/s]\n",
            "Training...:   1%|          | 15/2076 [24:20<50:47:09, 88.71s/it]\u001b[A\n",
            "Training...:   1%|          | 16/2076 [24:20<51:36:40, 90.19s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (15 | Loss: 3.5538675785064697, Learning Rate: 0.0002997688134200871)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [25:46<?, ?it/s]\n",
            "Training...:   1%|          | 16/2076 [25:46<51:36:40, 90.19s/it]\u001b[A\n",
            "Training...:   1%|          | 17/2076 [25:46<50:51:02, 88.91s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (16 | Loss: 3.491778612136841, Learning Rate: 0.0002997543488163501)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [27:10<?, ?it/s]\n",
            "Training...:   1%|          | 17/2076 [27:10<50:51:02, 88.91s/it]\u001b[A\n",
            "Training...:   1%|          | 18/2076 [27:10<49:59:29, 87.45s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (17 | Loss: 3.707183837890625, Learning Rate: 0.0002997398842126131)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [28:55<?, ?it/s]\n",
            "Training...:   1%|          | 18/2076 [28:55<49:59:29, 87.45s/it]\u001b[A\n",
            "Training...:   1%|          | 19/2076 [28:55<52:55:14, 92.62s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (18 | Loss: 3.3348066806793213, Learning Rate: 0.00029972544871270657)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [30:21<?, ?it/s]\n",
            "Training...:   1%|          | 19/2076 [30:21<52:55:14, 92.62s/it]\u001b[A\n",
            "Training...:   1%|          | 20/2076 [30:21<51:47:47, 90.69s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (19 | Loss: 3.843967914581299, Learning Rate: 0.00029971098410896957)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [31:45<?, ?it/s]\n",
            "Training...:   1%|          | 20/2076 [31:45<51:47:47, 90.69s/it]\u001b[A\n",
            "Training...:   1%|          | 21/2076 [31:45<50:34:43, 88.61s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (20 | Loss: 3.39107346534729, Learning Rate: 0.00029969654860906303)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [33:29<?, ?it/s]\n",
            "Training...:   1%|          | 21/2076 [33:29<50:34:43, 88.61s/it]\u001b[A\n",
            "Training...:   1%|          | 22/2076 [33:29<53:16:29, 93.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (21 | Loss: 3.344048023223877, Learning Rate: 0.00029968208400532603)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [34:55<?, ?it/s]\n",
            "Training...:   1%|          | 22/2076 [34:55<53:16:29, 93.37s/it]\u001b[A\n",
            "Training...:   1%|          | 23/2076 [34:55<51:59:15, 91.16s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (22 | Loss: 3.350283622741699, Learning Rate: 0.0002996676485054195)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [36:21<?, ?it/s]\n",
            "Training...:   1%|          | 23/2076 [36:21<51:59:15, 91.16s/it]\u001b[A\n",
            "Training...:   1%|          | 24/2076 [36:21<51:01:34, 89.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (23 | Loss: 3.263956308364868, Learning Rate: 0.0002996531839016825)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [38:09<?, ?it/s]\n",
            "Training...:   1%|          | 24/2076 [38:09<51:01:34, 89.52s/it]\u001b[A\n",
            "Training...:   1%|          | 25/2076 [38:09<54:14:39, 95.21s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (24 | Loss: 3.413923740386963, Learning Rate: 0.00029963874840177596)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [39:40<?, ?it/s]\n",
            "Training...:   1%|          | 25/2076 [39:40<54:14:39, 95.21s/it]\u001b[A\n",
            "Training...:   1%|▏         | 26/2076 [39:40<53:28:59, 93.92s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (25 | Loss: 3.178147554397583, Learning Rate: 0.00029962428379803896)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [41:06<?, ?it/s]\n",
            "Training...:   1%|▏         | 26/2076 [41:06<53:28:59, 93.92s/it]\u001b[A\n",
            "Training...:   1%|▏         | 27/2076 [41:06<52:04:48, 91.50s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (26 | Loss: 3.376338243484497, Learning Rate: 0.0002996098482981324)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [42:29<?, ?it/s]\n",
            "Training...:   1%|▏         | 27/2076 [42:29<52:04:48, 91.50s/it]\u001b[A\n",
            "Training...:   1%|▏         | 28/2076 [42:29<50:36:16, 88.95s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (27 | Loss: 3.436915874481201, Learning Rate: 0.0002995953836943954)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [44:12<?, ?it/s]\n",
            "Training...:   1%|▏         | 28/2076 [44:12<50:36:16, 88.95s/it]\u001b[A\n",
            "Training...:   1%|▏         | 29/2076 [44:12<52:56:17, 93.10s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (28 | Loss: 3.3066864013671875, Learning Rate: 0.0002995809481944889)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [45:47<?, ?it/s]\n",
            "Training...:   1%|▏         | 29/2076 [45:47<52:56:17, 93.10s/it]\u001b[A\n",
            "Training...:   1%|▏         | 30/2076 [45:47<53:10:55, 93.58s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (29 | Loss: 3.501091957092285, Learning Rate: 0.0002995664835907519)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [47:17<?, ?it/s]\n",
            "Training...:   1%|▏         | 30/2076 [47:17<53:10:55, 93.58s/it]\u001b[A\n",
            "Training...:   1%|▏         | 31/2076 [47:17<52:33:52, 92.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (30 | Loss: 3.1216554641723633, Learning Rate: 0.00029955204809084535)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [49:01<?, ?it/s]\n",
            "Training...:   1%|▏         | 31/2076 [49:01<52:33:52, 92.53s/it]\u001b[A\n",
            "Training...:   2%|▏         | 32/2076 [49:01<54:28:09, 95.93s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (31 | Loss: 3.507996082305908, Learning Rate: 0.00029953758348710835)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [50:24<?, ?it/s]\n",
            "Training...:   2%|▏         | 32/2076 [50:24<54:28:09, 95.93s/it]\u001b[A\n",
            "Training...:   2%|▏         | 33/2076 [50:24<52:19:02, 92.19s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (32 | Loss: 3.3109123706817627, Learning Rate: 0.0002995231479872018)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [51:49<?, ?it/s]\n",
            "Training...:   2%|▏         | 33/2076 [51:49<52:19:02, 92.19s/it]\u001b[A\n",
            "Training...:   2%|▏         | 34/2076 [51:49<51:04:32, 90.05s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (33 | Loss: 3.6426773071289062, Learning Rate: 0.0002995086833834648)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [53:32<?, ?it/s]\n",
            "Training...:   2%|▏         | 34/2076 [53:32<51:04:32, 90.05s/it]\u001b[A\n",
            "Training...:   2%|▏         | 35/2076 [53:32<53:18:29, 94.03s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (34 | Loss: 3.3632867336273193, Learning Rate: 0.0002994942478835583)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [54:56<?, ?it/s]\n",
            "Training...:   2%|▏         | 35/2076 [54:56<53:18:29, 94.03s/it]\u001b[A\n",
            "Training...:   2%|▏         | 36/2076 [54:56<51:30:33, 90.90s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (35 | Loss: 3.400197982788086, Learning Rate: 0.0002994797832798213)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [56:21<?, ?it/s]\n",
            "Training...:   2%|▏         | 36/2076 [56:21<51:30:33, 90.90s/it]\u001b[A\n",
            "Training...:   2%|▏         | 37/2076 [56:21<50:30:49, 89.19s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (36 | Loss: 3.3949944972991943, Learning Rate: 0.0002994653186760843)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [58:05<?, ?it/s]\n",
            "Training...:   2%|▏         | 37/2076 [58:05<50:30:49, 89.19s/it]\u001b[A\n",
            "Training...:   2%|▏         | 38/2076 [58:05<52:57:50, 93.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (37 | Loss: 3.394672155380249, Learning Rate: 0.00029945088317617774)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [59:28<?, ?it/s]\n",
            "Training...:   2%|▏         | 38/2076 [59:28<52:57:50, 93.56s/it]\u001b[A\n",
            "Training...:   2%|▏         | 39/2076 [59:28<51:10:54, 90.45s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (38 | Loss: 3.1371517181396484, Learning Rate: 0.00029943641857244074)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch ... :   0%|          | 0/10 [1:00:53<?, ?it/s]\n",
            "Training...:   2%|▏         | 39/2076 [1:00:53<51:10:54, 90.45s/it]\u001b[A\n",
            "Training...:   2%|▏         | 40/2076 [1:00:53<50:14:45, 88.84s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step... (39 | Loss: 3.1099400520324707, Learning Rate: 0.0002994219830725342)\n",
            "model_inputs are {'input_ids': (20, 512), 'attention_mask': (20, 512), 'decoder_input_ids': (20, 512), 'decoder_attention_mask': (20, 512), 'labels': (20, 512)}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train_time = 0\n",
        "epochs = tqdm.tqdm(range(num_epochs), desc=\"Epoch ... \", position=0)\n",
        "for epoch in epochs:\n",
        "    # ======================== Training ================================\n",
        "    train_start = time.time()\n",
        "    train_metrics = []\n",
        "\n",
        "    # Create sampling rng\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "    # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "    num_train_samples = len(train_data['input_ids'])\n",
        "    train_samples_idx = jax.random.permutation(input_rng, jnp.arange(num_train_samples))\n",
        "    #train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n",
        "\n",
        "    # Gather the indexes for creating the batch and do a training step\n",
        "    for step, batch_idx in enumerate(tqdm.tqdm(range(num_train_samples // train_batch_size), desc=\"Training...\", position=1)):\n",
        "        #samples = [tokenized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n",
        "        model_inputs = batch_from_indices(train_data, train_samples_idx[train_batch_size * batch_idx:train_batch_size * batch_idx + train_batch_size])\n",
        "        #print('model_inputs are', {key:val.shape for key, val in model_inputs.items()})\n",
        "        #model_inputs = data_collator(samples)\n",
        "\n",
        "        # Model forward\n",
        "        model_inputs = flax.training.common_utils.shard(model_inputs)\n",
        "        state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rng=dropout_rngs)\n",
        "        train_metrics.append(train_metric)\n",
        "\n",
        "        cur_step = epoch * (num_train_samples // train_batch_size) + step\n",
        "\n",
        "        if cur_step % training_args.logging_steps == 0 and cur_step > 0 and jax.process_index() == 0:\n",
        "            # Save metrics\n",
        "            train_metric = flax.jax_utils.unreplicate(train_metric)\n",
        "            train_time += time.time() - train_start\n",
        "            #if has_tensorboard and jax.process_index() == 0:\n",
        "            #    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n",
        "\n",
        "            epochs.write(\n",
        "                f\"Loss: {train_metric['loss'].mean()}, Learning Rate: {train_metric['learning_rate'].mean()}\"\n",
        "            )\n",
        "\n",
        "            train_metrics = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGdYRKVJxHxN"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  print(repr(eval(input('>>> '), globals(), locals())))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K2lLdDyZP0mT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "background_execution": "on",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}