{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xloem/techsketball/blob/main/model_import_sketch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH1Ld_vd9wyx",
        "outputId": "09e1efa4-cc4f-4629-883d-f5aa1b767625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.7/dist-packages (0.5.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.62.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.10.2.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed) (21.3)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed) (5.4.8)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.7/dist-packages (from deepspeed) (3.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deepspeed) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from triton->deepspeed) (3.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: jax>=0.2.21 in /usr/local/lib/python3.7/dist-packages (from flax) (0.2.25)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.21->flax) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.71+cuda111)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax->flax) (2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "fatal: destination path 'techsketball' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "starting_model_path = 't5-base'#'bigscience/T0pp'\n",
        "input_width = 512\n",
        "\n",
        "\n",
        "#!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip3 install deepspeed\n",
        "!pip3 install transformers\n",
        "!pip3 install flax\n",
        "!pip3 install sentencepiece\n",
        "!git clone https://github.com/xloem/techsketball && ln -s techsketball/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mTGBjrXoX2eS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "a8184726-342d-4d3c-e0bd-743f1891e9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d88f3a382c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_tpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_tpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_tpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py\u001b[0m in \u001b[0;36mlocal_devices\u001b[0;34m(process_index, backend, host_id)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mprocess_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mprocess_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0mprocess_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mprocess_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprocess_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown process_index {process_index}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py\u001b[0m in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# don't use util.memoize because there is no X64 dependence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py\u001b[0m in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mplatform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_backends_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         raise RuntimeError(f\"Backend '{platform}' failed to initialize: \"\n\u001b[0m\u001b[1;32m    275\u001b[0m                            f\"{_backends_errors[platform]}\")\n\u001b[1;32m    276\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown backend {platform}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Backend 'tpu_driver' failed to initialize: DEADLINE_EXCEEDED: Failed to connect to remote server at address: grpc://10.16.179.218:8470. Error from gRPC: Deadline Exceeded. Details: "
          ]
        }
      ],
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "jax.local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhJhFJfQAOXG"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, FlaxT5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(starting_model_path) # only for source, not for binary\n",
        "model = FlaxT5ForConditionalGeneration.from_pretrained(starting_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnyDTDt_f1fE"
      },
      "outputs": [],
      "source": [
        "from find_pycode import pair_finder\n",
        "print('generating training data ...')\n",
        "data_tuples = [*pair_finder(globals())] # finds code examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrRadfeAhZSd",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import jax, numpy as np\n",
        "data_input_ids = jax.numpy.stack([\n",
        "  np.frombuffer(bytes.ljust(input_width, b'\\0'), dtype=np.uint8)\n",
        "  for bytes, str in data_tuples\n",
        "  if len(bytes) <= input_width\n",
        "])\n",
        "data_labels = [str for bytes, str in data_tuples if len(bytes) <= input_width]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qTNv8oZwbGS"
      },
      "outputs": [],
      "source": [
        "#from tokenizers import ByteLevelBPETokenizer\n",
        "#tokenizer = ByteLevelBPETokenizer()\n",
        "#tokenizer.train_from_iterator((str for bytes, str in data_tuples), vocab_size=model.config.vocab_size, min_frequency=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxCgcJ0dzQY_"
      },
      "outputs": [],
      "source": [
        "data_labels_tokenized = tokenizer(data_labels, padding = True, return_tensors = 'np')\n",
        "data_label_ids = data_labels_tokenized['input_ids']\n",
        "data_label_attention_masks = data_labels_tokenized['attention_mask']\n",
        "#data_label_ids[data_label_attention_masks == 0] = -100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ ==> i think data batches are to be dicts with keys: input_ids labels decoder_attention_mask\n",
        "############     another possible key is decoder_input_ids\n",
        "tokenized_datasets = {\n",
        "    'train':{\n",
        "        'input_ids': data_input_ids,\n",
        "        'labels': data_label_ids,\n",
        "        'decoder_attention_mask': data_label_attention_masks\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "5yoFz6-OegdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdJ1Ek3-_j37"
      },
      "outputs": [],
      "source": [
        "#import deepspeed\n",
        "#cmd_args = None\n",
        "#model_engine, optimizer, _, _ = deepspeed.initialize(args=cmd_args,\n",
        "#                                                     model=model,\n",
        "#                                                     model_parameters=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTQyR1KO4Sbv"
      },
      "outputs": [],
      "source": [
        "# these are not t5 parameters\n",
        "per_device_batch_size = 16 # small for notebook\n",
        "num_epochs = 10\n",
        "training_seed = 0\n",
        "learning_rate = 3e-4\n",
        "\n",
        "total_batch_size = per_device_batch_size * jax.device_count()\n",
        "num_train_steps = len(data_input_ids) // total_batch_size * num_epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gD_cvz5MebpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doKKw-W345Zn",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "import optax, flax.training.train_state\n",
        "linear_decay_lr_schedule_fn = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)\n",
        "adamw = optax.adamw(learning_rate=linear_decay_lr_schedule_fn, b1=0.9, b2=0.98, eps=1e-8, weight_decay=0.01)\n",
        "state = flax.training.train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw)\n",
        "\n",
        "def data_loader(rng, dataset, batch_size, shuffle=True):\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "\n",
        "    if shuffle:\n",
        "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
        "    else:\n",
        "        batch_idx = jnp.arange(len(dataset))\n",
        "\n",
        "    batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for idx in batch_idx:\n",
        "        batch = dataset[idx]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch\n",
        "\n",
        "# from run_t5_mlm_flax.py\n",
        "rng = jax.random.PRNGKey(training_seed)\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "# Define gradient update step fn\n",
        "def train_step(state, batch, dropout_rng):\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "    def loss_fn(params):\n",
        "        #labels = batch.pop(\"labels\")\n",
        "\n",
        "        #forward_pass = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)\n",
        "\n",
        "        # compute loss\n",
        "        #loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])).mean()\n",
        "\n",
        "        loss = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True).loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    metrics = jax.lax.pmean(\n",
        "        {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n",
        "    )\n",
        "\n",
        "    return new_state, metrics, new_dropout_rng\n",
        "\n",
        "# Create parallel version of the train step\n",
        "p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n",
        "\n",
        "# Replicate the train state on each device\n",
        "state = flax.jax_utils.replicate(state)\n",
        "\n",
        "train_time = 0\n",
        "epochs = tqdm(range(num_epochs), desc=\"Epoch ... \", position=0)\n",
        "for epoch in epochs:\n",
        "    # ======================== Training ================================\n",
        "    train_start = time.time()\n",
        "    train_metrics = []\n",
        "\n",
        "    # Create sampling rng\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "    # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "    num_train_samples = len(tokenized_datasets[\"train\"])\n",
        "    train_samples_idx = jax.random.permutation(input_rng, jnp.arange(num_train_samples))\n",
        "    train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n",
        "\n",
        "    # Gather the indexes for creating the batch and do a training step\n",
        "    for step, batch_idx in enumerate(tqdm(train_batch_idx, desc=\"Training...\", position=1)):\n",
        "        samples = [tokenized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n",
        "        model_inputs = data_collator(samples)\n",
        "\n",
        "        # Model forward\n",
        "        model_inputs = shard(model_inputs.data)\n",
        "        state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rngs)\n",
        "        train_metrics.append(train_metric)\n",
        "\n",
        "        cur_step = epoch * (num_train_samples // train_batch_size) + step\n",
        "\n",
        "        if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n",
        "            # Save metrics\n",
        "            train_metric = jax_utils.unreplicate(train_metric)\n",
        "            train_time += time.time() - train_start\n",
        "            if has_tensorboard and jax.process_index() == 0:\n",
        "                write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n",
        "\n",
        "            epochs.write(\n",
        "                f\"Step... ({cur_step} | Loss: {train_metric['loss'].mean()}, Learning Rate: {train_metric['learning_rate'].mean()})\"\n",
        "            )\n",
        "\n",
        "            train_metrics = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGdYRKVJxHxN"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  print(repr(eval(input('>>> '), globals(), locals())))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}