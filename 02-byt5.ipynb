{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xloem/techsketball/blob/main/02-byt5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH1Ld_vd9wyx",
        "outputId": "9783c69e-1914-4a7c-9f9c-2f05fc718ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.25)\n",
            "Collecting jax\n",
            "  Downloading jax-0.2.28.tar.gz (887 kB)\n",
            "\u001b[K     |████████████████████████████████| 887 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax) (1.4.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from jax) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.2.28-py3-none-any.whl size=1028669 sha256=bb649cb4242576472fcdbe1bb98fcbd6464327801a2a9b51df1858007a7f13e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/60/4c/0cf931b766116b73950d9b6fca5813a45789d45d412a8d7272\n",
            "Successfully built jax\n",
            "Installing collected packages: jax\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.2.25\n",
            "    Uninstalling jax-0.2.25:\n",
            "      Successfully uninstalled jax-0.2.25\n",
            "Successfully installed jax-0.2.28\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.71+cuda111)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.1.76-cp37-none-manylinux2010_x86_64.whl (65.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 65.1 MB 167 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib) (2.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jaxlib) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.1.71+cuda111\n",
            "    Uninstalling jaxlib-0.1.71+cuda111:\n",
            "      Successfully uninstalled jaxlib-0.1.71+cuda111\n"
          ]
        }
      ],
      "source": [
        "#[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "starting_model_path = 'baffo32/pyc2py_alpha2'#'google/byt5-large' #'t5-base'#'t5-small'#'bigscience/T0pp'\n",
        "train_tokenizer = False # baffo32/pyc2py_alpha already has a tokenizer that can output bytes\n",
        "\n",
        "input_width = 512\n",
        "# these are not t5 parameters?\n",
        "train_batch_size = 8 # small for notebook\n",
        "num_epochs = 1\n",
        "training_seed = 0\n",
        "learning_rate = 0.0001#3e-4\n",
        "logging_steps = 128\n",
        "\n",
        "\n",
        "#!pip3 install --upgrade jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip3 install --upgrade jax\n",
        "!pip3 install --upgrade jaxlib\n",
        "!pip3 install deepspeed\n",
        "!pip3 install transformers\n",
        "!pip3 install flax\n",
        "!pip3 install sentencepiece\n",
        "!git clone https://github.com/xloem/techsketball && ln -s techsketball/* .\n",
        "!apt-get install git-lfs\n",
        "!git config --global user.email johndoe@example.com\n",
        "!git config --global user.name 'John Doe'\n",
        "!git config --global credential.helper store\n",
        "\n",
        "!git-lfs install\n",
        "!git clone https://huggingface.co/baffo32/pyc2py_alpha2 local_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTGBjrXoX2eS"
      },
      "outputs": [],
      "source": [
        "import jax, jax.tools.colab_tpu, jaxlib\n",
        "import os\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "  jaxlib.tpu_client_extension.TpuClient.runtime_type = 'stream_executor'\n",
        "  backend = 'tpu'\n",
        "else:\n",
        "  backend = jax.default_backend()\n",
        "import tensorflow as tf\n",
        "per_device_batch_size = train_batch_size // jax.device_count() ############################\n",
        "!nvidia-smi\n",
        "jax.local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhJhFJfQAOXG"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, FlaxT5ForConditionalGeneration \n",
        "import huggingface_hub\n",
        "repo = huggingface_hub.Repository('local_model', clone_from=starting_model_path)\n",
        "#try:\n",
        "#    tokenizer = T5Tokenizer.from_pretrained('local_model')\n",
        "#except:\n",
        "#    tokenizer = T5Tokenizer.from_pretrained(starting_model_path)\n",
        "try:\n",
        "    model = FlaxT5ForConditionalGeneration.from_pretrained('local_model')\n",
        "except:\n",
        "    model = FlaxT5ForConditionalGeneration.from_pretrained(starting_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_call = jax.jit(model)"
      ],
      "metadata": {
        "id": "dogwWZHAvocq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZqc_JNhixjS"
      },
      "outputs": [],
      "source": [
        "# before data is generated we can import libraries to generate it from\n",
        "import jax, jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import flax\n",
        "import flax.training.common_utils\n",
        "import flax.training.train_state\n",
        "import tqdm\n",
        "import time\n",
        "import os\n",
        "# ...\n",
        "import transformers\n",
        "import deepspeed\n",
        "import scipy\n",
        "import matplotlib\n",
        "import sentencepiece\n",
        "import dis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnyDTDt_f1fE"
      },
      "outputs": [],
      "source": [
        "import find_pycode\n",
        "print('getting training data ...')\n",
        "#tokenizerpfx = starting_model_path.replace('/','_') + '.'\n",
        "#if not train_tokenizer:\n",
        "#  import bytepreservingsentencepiece\n",
        "#  tokenizer.sp_model = bytepreservingsentencepiece.SentencePieceProcessor(tokenizer.vocab_file, **tokenizer.sp_model_kwargs)\n",
        "tokenizerpfx = ''\n",
        "find_pycode.write_files('example.', tokenizerpfx, 512, None, 512, globals(), skip_if_exists = True, tokenize_binary = False, tokenize_source = False)\n",
        "#if train_tokenizer:\n",
        "#  tokenizer.save_pretrained('local_model')\n",
        "# repo.push_to_hub(commit_message=f'commit-message', blocking=False)\n",
        "train_data = find_pycode.read_files('example.', tokenizerpfx, 512, 512, tokenize_binary = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qTNv8oZwbGS"
      },
      "outputs": [],
      "source": [
        "#from tokenizers import ByteLevelBPETokenizer\n",
        "#tokenizer = ByteLevelBPETokenizer()\n",
        "#tokenizer.train_from_iterator((str for bytes, str in data_tuples), vocab_size=model.config.vocab_size, min_frequency=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxCgcJ0dzQY_"
      },
      "outputs": [],
      "source": [
        "jax.config.update('jax_log_compiles', True)\n",
        "from jax.experimental.compilation_cache import compilation_cache\n",
        "#compilation_cache.initialize_cache('local_model/cc', max_cache_size_bytes=32*2**30)\n",
        "compilation_cache._cache = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdJ1Ek3-_j37"
      },
      "outputs": [],
      "source": [
        "#cmd_args = None\n",
        "#model_engine, optimizer, _, _ = deepspeed.initialize(args=cmd_args,\n",
        "#                                                     model=model,\n",
        "#                                                     model_parameters=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTQyR1KO4Sbv"
      },
      "outputs": [],
      "source": [
        "num_train_steps = len(train_data['input_ids']) // train_batch_size * num_epochs\n",
        "\n",
        "rng = jax.random.PRNGKey(training_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD_cvz5MebpC"
      },
      "outputs": [],
      "source": [
        "def batch_from_indices(dataset : dict, indices):\n",
        "  #print(dataset['input_ids'].shape, indices.shape)\n",
        "  result = {\n",
        "      key : jnp.stack(data[indices,:])\n",
        "      for key, data in dataset.items()\n",
        "  }\n",
        "  # this change could be already put in the dataset passed by the function that produces it\n",
        "  result['labels'] = result['decoder_input_ids']\n",
        "  pad_token_id = 0 # tokenizer.pad_token_id\n",
        "  result['decoder_input_ids'] = np.asarray(transformers.models.t5.modeling_flax_t5.shift_tokens_right(result['labels'], pad_token_id, model.config.decoder_start_token_id))\n",
        "  return result\n",
        "\n",
        "# are these t5 parameters?\n",
        "linear_decay_lr_schedule_fn = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)\n",
        "adamw = optax.adamw(learning_rate=linear_decay_lr_schedule_fn, b1=0.9, b2=0.98, eps=1e-8, weight_decay=0.01)\n",
        "state = flax.training.train_state.TrainState.create(apply_fn=model_call, params=model.params, tx=adamw)\n",
        "\n",
        "# Replicate the train state on each device\n",
        "p_state = flax.jax_utils.replicate(state)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separate forward function out during compilation\n",
        "jitted_state_apply_fn = jax.jit(state.apply_fn)\n",
        "jitted_p_state_apply_fn = jax.jit(p_state.apply_fn)"
      ],
      "metadata": {
        "id": "ERVfFKgPuNAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m58ESSevKp6P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from run_t5_mlm_flax.py\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "# Define gradient update step fn\n",
        "def train_step(state, batch, dropout_rng):#input_ids, attention_mask, labels, decoder_input_ids, decoder_attention_mask, dropout_rng):\n",
        "    print('Compiling train_step first line')\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "    def loss_fn(params):\n",
        "        print('Compiling loss_fn.train_step first line')\n",
        "        labels = batch.pop('labels')\n",
        "\n",
        "        #logits = state.apply_fn(\n",
        "        #    input_ids = input_ids,\n",
        "        #    attention_mask = attention_mask,\n",
        "        #    decoder_input_ids = decoder_input_ids,\n",
        "        #    decoder_attention_mask = decoder_attention_mask,\n",
        "        #    params = params,\n",
        "        #    dropout_rng = dropout_rng,\n",
        "        #    train = True\n",
        "        #).logits\n",
        "        logits = state.apply_fn(**batch, params = params, dropout_rng = dropout_rng, train = True).logits\n",
        "        #print(logits.shape)\n",
        "        #assert len(logits[-1]) == tokenizer.vocab_size\n",
        "        #logits = logits[0]\n",
        "\n",
        "        # logits, labels, padding_mask=batch['decoder_attention_mask', label_smoothing_factor=0]\n",
        "        # compute loss\n",
        "        print('Compiling train_step.loss_fn loss')\n",
        "        loss = optax.softmax_cross_entropy(logits, flax.training.common_utils.onehot(labels, logits.shape[-1]))\n",
        "        print('Compiling train_step.loss_fn padding_mask')\n",
        "        padding_mask = batch['decoder_attention_mask']\n",
        "        loss = (loss * padding_mask).sum() / padding_mask.sum()\n",
        "\n",
        "        #loss = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True).loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    print('Compiling train_step value_and_grad call')\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    print('Compiling train_step grad_fn')\n",
        "    loss, grad = grad_fn(state.params)\n",
        "    print('Compiling train_step pmean')\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    print('Compiling train_step apply_gradients')\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    print('Compiling train_step metrics')\n",
        "    metrics = jax.lax.pmean(\n",
        "        {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n",
        "    )\n",
        "\n",
        "    return new_state, metrics, new_dropout_rng\n",
        "\n",
        "# Create parallel version of the train step\n",
        "p_train_step = jax.pmap(train_step, 'batch', donate_argnums=(0,), backend=backend)\n",
        "\n",
        "print('Performing initial batchs to compile train step ...')\n",
        "rng, input_rng = jax.random.split(rng)\n",
        "num_train_samples = len(train_data['input_ids'])\n",
        "train_samples_idx = jax.random.permutation(input_rng, jnp.arange(num_train_samples))\n",
        "model_inputs = batch_from_indices(train_data, train_samples_idx[:train_batch_size])\n",
        "\n",
        "# trying single device pass\n",
        "state, train_metric, dropout_rngs = train_step(state, model_inputs, dropout_rng=dropout_rngs)\n",
        "\n",
        "model_inputs = flax.training.common_utils.shard(model_inputs)\n",
        "print('About to call p_train_step ...')\n",
        "p_state, train_metric, dropout_rngs = p_train_step(p_state, model_inputs, dropout_rng=dropout_rngs)\n",
        "train_metric = flax.jax_utils.unreplicate(train_metric)\n",
        "print('Done.  First loss was', train_metric['loss'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doKKw-W345Zn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_time = 0\n",
        "epochs = tqdm.tqdm(range(num_epochs), desc=\"Epoch ... \", position=0)\n",
        "for epoch in epochs:\n",
        "    # ======================== Training ================================\n",
        "    train_start = time.time()\n",
        "    train_metrics = []\n",
        "\n",
        "    # Create sampling rng\n",
        "    rng, input_rng = jax.random.split(rng)\n",
        "\n",
        "    # Generate an epoch by shuffling sampling indices from the train dataset\n",
        "    num_train_samples = len(train_data['input_ids'])\n",
        "    train_samples_idx = jax.random.permutation(input_rng, jnp.arange(num_train_samples))\n",
        "    #train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n",
        "\n",
        "    # Gather the indexes for creating the batch and do a training step\n",
        "    batches_tqdm = tqdm.tqdm(range(num_train_samples // train_batch_size), desc=\"Training...\", position=1)\n",
        "    for step, batch_idx in enumerate(batches_tqdm):\n",
        "        #samples = [tokenized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n",
        "        model_inputs = batch_from_indices(train_data, train_samples_idx[train_batch_size * batch_idx:train_batch_size * batch_idx + train_batch_size])\n",
        "        #print('model_inputs are', {key:val.shape for key, val in model_inputs.items()})\n",
        "        #model_inputs = data_collator(samples)\n",
        "\n",
        "        # Model forward\n",
        "        model_inputs = flax.training.common_utils.shard(model_inputs)\n",
        "        p_state, train_metric, dropout_rngs = p_train_step(p_state, model_inputs, dropout_rng=dropout_rngs)\n",
        "        train_metrics.append(train_metric)\n",
        "\n",
        "        cur_step = epoch * (num_train_samples // train_batch_size) + step\n",
        "\n",
        "        if cur_step % logging_steps == 0 and cur_step > 0 and jax.process_index() == 0:\n",
        "            # Save metrics\n",
        "            train_metric = flax.jax_utils.unreplicate(train_metric)\n",
        "            train_time += time.time() - train_start\n",
        "            #if has_tensorboard and jax.process_index() == 0:\n",
        "            #    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n",
        "\n",
        "            epochs.write(\n",
        "                f\"Loss: {train_metric['loss'].mean()}, Learning Rate: {train_metric['learning_rate'].mean()}\"\n",
        "            )\n",
        "\n",
        "            train_metrics = []\n",
        "        #if cur_step % (num_train_samples // 8) == 0:\n",
        "            # save checkpoint\n",
        "            if jax.process_index() == 0:\n",
        "                params = jax.device_get(jax.tree_map(lambda x: x[0], state.params))\n",
        "                model.save_pretrained('local_model', params=params)\n",
        "                repo.push_to_hub(commit_message=f'step={step} loss={train_metric[\"loss\"].mean()}', blocking=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGdYRKVJxHxN"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  print(repr(eval(input('>>> '), globals(), locals())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2lLdDyZP0mT"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of Untitled3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}